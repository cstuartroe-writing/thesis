\chapter{Introduction and overview}
\label{introduction}

In linguistics, \textbf{morphology} refers to alterations to words to reflect changes in meaning or grammatical category. For example, English verbs have differing morphological forms to indicate the simple present and simple past tenses, e.g., \textit{show} $\rightarrow$ \textit{showed}, \textit{see} $\rightarrow$ \textit{saw}, etc. \parencite{Dreyer2008}. Grammatical inflection in particular has a tendency to be structured into \textbf{paradigms} - sets of all possible morphological forms that words of a certain type can take on, often shown arrayed in tables. The table below gives part of the paradigm for the Spanish adjective \textit{pequeño} "large":

\begin{center}
\begin{tabular}{|c||c|c|}
\hline
& singular & plural \\
\hline \hline
masculine & \textit{pequeño} & \textit{pequeños} \\
\hline 
feminine & \textit{pequeña} & \textit{pequeñas} \\
\hline
\end{tabular}
\end{center}

Historically, in language technologies and modeling, morphology has been somewhat under-emphasized. This is probably due at least in part to the dominance of English in language technology research, and its below-average morphological complexity \parencite{Cotterell2017a}. English lexemes tend to have few grammatically inflected forms compared to in most other languages. This means that in machine learning models which are given an English training corpus and then tested on new material, the occurrence of \textbf{out-of-vocabulary (OOV)} inflected forms - that is, forms in the test data that never occur in the training data - are less frequent than in some other languages. 

In a more morphologically complex language, a text may contain many inflected forms that are individually rarer but nonetheless perfectly intelligible to a person or model that understands the morphology. For example, a person learning Spanish may have never encountered the form \textit{pequeñas} before, but if they have seen all three other forms of the word and are familiar with Spanish adjectival morphology, they will have no difficulty in fully understanding the meaning of the word. If language models can behave similarly rather than treating a new word like \textit{pequeñas} as OOV - that is, fundamentally unknown - their understanding of new material in morphologically complex languages may be substantially enhanced \parencite{Cotterell2016}. It has been empirically shown that comprehending grammatical categories and inflection improves accuracy rates in language modeling and machine translation \parencite{Faruqui2015}.

The state of the art for modeling morphology since 2016 has been variants of a model type called \textbf{long short-term memory (LSTM) neural networks}, briefly described in \ref{sec:LSTM}, operating over individual characters and grammatical category tags. Their use for computational morphology has been pioneered by SIGMORPHON, a research group that holds annual \textit{shared tasks}, competitions among several research teams on a morphology prediction problem. LSTMs definitively overtook the field after their strong performance relative to other model types in the SIGMORPHON 2016 shared task. However, learning curve analysis in the SIGMORPHON 2017 task showed that LSTMs perform well in high-data settings but, provided with lower volumes of training data, actually fare worse than simpler model types trained on similar amounts of data \parencite{Cotterell2017a}. In the 2018 shared task, an identical morphology prediction task with data for more languages, the learning curve issue was addressed to some degree by \textbf{ensembling} - a means of using the output of several models at once - with other methods \parencite{Cotterell2018b}. For languages with low quantities of digital resources, though, there is still much room for improvement of computational morphology models.

One of the SIGMORPHON 2019 shared tasks focused on \textbf{transfer learning}, leveraging a high volume of data for one language to better model the morphology of another language for which a low volume of data was provided. The research teams tested their models on 100 transfer learning pairs, of which 80 were closely related languages. They claim that this use of data about other languages produced "modest" model performance gains, with data transferred from related languages being more conducive to strengthening models \parencite{McCarthy2019}. However, some of those conclusions are challenged, or shown to be more complicated, in this study. 

Also, even if a model of a particular language can be strengthened using data about a related language, there are plenty of low-resource languages not closely related to other high-resource languages. It may be worthwhile to assess what other linguistic properties of a language may predict its usefulness for improving modeling of other languages. 

In this study, a rudimentary data set of linguistic characteristics of the languages used in SIGMORPHON 2019 was compiled, and statistical tests were conducted to assess which of those linguistic properties predict transfer learning success. From these results, it seems apparent that two languages having similar systems of \textit{verbs} is predictive of transfer learning yielding model improvements, while other types of similarity have a less clear influence. Further, the data is suggestive that transfer learning between related languages may actually be a hindrance to model performance. However, the nature of language pair selection for SIGMORPHON 2019 seems to have introduced some confounding skews in their data which make all of the inferences of this study more tenuous. In order to confidently assess the relationships between various linguistic properties and transfer learning success, future work could be conducted that specifically uses language pairs with a variety of linguistic similarities and differences from one another. This study also provides recommendations for new types of linguistic information that may be compiled, to widen the search space of this line of research and better guarantee that any further investigation of transfer learning uses a linguistically diverse set of language pairs.