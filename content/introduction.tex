\chapter{Introduction and overview}
\label{introduction}

In linguistics, morphology refers to alterations to words to reflect changes in meaning or grammatical category. For example, English verbs have differing morphological forms to indicate the simple present and simple past tenses, e.g., \textit{show} $\rightarrow$ \textit{showed}, \textit{see} $\rightarrow$ \textit{saw}, etc. \parencite{Dreyer2008}. Grammatical inflection in particular has a tendency to be structured into \textbf{paradigms} - sets of all possible morphological forms that words of a certain type can take on, often shown arrayed in tables. The table below gives part of the paradigm for the verb \textit{to see}:

\begin{tabular}{|c||c|c|c|c|}
\hline
& \multicolumn{2}{c|}{simple} & \multicolumn{2}{c|}{progressive} \\
\hline
& 3rd singular & 3rd plural & 3rd singular & 3rd plural \\
\hline \hline
present & (she) \textbf{sees} & (they) \textbf{see} & (she) \textbf{is seeing} & (they) \textbf{are seeing} \\
\hline 
past & (she) \textbf{saw} & (they) \textbf{saw} & (she) \textbf{was seeing} & (they) \textbf{were seeing} \\
\hline
\end{tabular}

Historically, in language technologies and modeling, morphology has been somewhat under-emphasized. This is probably due at least in part to the dominance of English in language technology research, and its below-average morphological complexity \parencite{Cotterell2017a}. English lexemes tend to have few grammatically inflected forms compared to in most other languages. This means that in machine learning models which are given an English training corpus and then tested on new material, the occurrence of \textit{out-of-vocabulary} (OOV) inflected forms - that is, forms in the test data that never occur in the training data - are less frequent. 

Moreover, as can be seen above, many English grammatical forms are periphrastic, using several words to construct a single inflected form. This means that even with grammatical information removed via lemmatization, the process of reducing words to their citation form, an English sentence may still be fairly understandable. (The \textit{citation form} of a lexeme is the form under which it would appear in a dictionary: \textit{see}, \textit{sees}, \textit{seeing}, and \textit{saw} all belong to the same lexeme, with the citation form \textit{see}.) For instance, given a sentence \textit{she have see it} whose words have been lemmatized, a proficient reader of English can guess that the original sentence was \textit{she has seen it} or \textit{she had seen it}; not much information is lost.

However, in many languages grammatical inflection carries much more semantic burden, and the number of possible inflected forms can be much greater. For instance, a single verb in the Archi language can be inflected in 1,725 ways \parencite{Kibrik1994}. For such highly inflected languages, data sparsity is a significant problem for language models naive to morphology. In languages with a high number of possible forms per lexeme, a much larger number of OOV forms are inevitably encountered in test data, requiring reliance on a model's representation of OOV words. Even inflected forms that do occur in training data may not appear often enough for a naive model to understand their semantic content. A model that considers grammatical categories separately is better able to understand the semantic content of inflected words \parencite{Cotterell2016}. It has been empirically shown that comprehending grammatical categories and inflection improves accuracy rates in language modeling and machine translation \parencite{Faruqui2015}.

The state of the art for modeling morphology since 2016 has been variants of long short-term memory (LSTM) neural networks and the similar gated recurrent unit (GRU) architecture, operating over individual characters and grammatical category tags. These definitively overtook the field after their strong performance relative to traditional string transduction models in the SIGMORPHON 2016 shared task, a competition among several research teams on a morphology prediction problem. However, learning curve analysis in the SIGMORPHON 2017 task showed that such models, in particular encoder-decoder variants, perform well in high-data settings but, provided with lower volumes of training data, actually fare worse than string transduction models trained on similar amounts of data. To some degree, this poor performance could be ameliorated by pre-training on synthetic data, although this technique worked significantly better for languages with small inflectional paradigms \parencite{Cotterell2017a}. In the 2018 shared task, an identical morphology prediction task with data for more languages, the learning curve issue was addressed to some degree by ensembling with string transduction methods \parencite{Cotterell2018b}, but for languages with low quantities of digital resources, there is still much room for improvement of computational morphology models.

One of the 2019 shared tasks focused on utilizing transfer learning to strengthen neural morphology models, using pretraining or combined models to leverage a high volume of data for one language to better model the morphology of another language for which a low volume of data was provided. The research teams tested their models on 100 transfer learning pairs, of which 80 were closely related languages. Transfer learning produced "modest" performance gains over models without access to transfer learning, with related pairs showing significantly more model improvement than unrelated pairs \parencite{McCarthy2019}.

For some low-resource languages closely related to some high-resource language, the outcome of SIGMORPHON 2019 promises improved morphology modeling. But for the many low-resource languages may not closely related to other high-resource languages, it may be worthwhile to assess whether transfer learning can still be useful. It may be that pretraining on a typologically similar language, or simply on some other language(s) to understand the universal patterns of natural language morphology, can help neural models make more of sparse morphological data for low-resource languages. To this end, I'm planning on investigating what typological commonalities are predictive of more effective transfer learning of morphology between languages.

In order to accomplish this, I will need to compile a dataset of morphological typological characteristics of the languages I wish to work with, alongside their morphology data. SIGMORPHON 2018 published an annotated morphological data set for 103 languages, including various training data volumes as well as test data, which I will use for morphology learning; I'll compile typological information from an existing database, the World Atlas of Language Structures (WALS), and via string transduction analyses of the SIGMORPHON data. From there, I'll assess the efficacy of transfer learning between a large number of language pairs, and what relationship that has with various types of typological similarity between the languages. I'll also seek to answer some related questions, such as whether transfer learning efficacy is typically reciprocal between two languages, how the efficacy of transfer learning interacts with learning curves, and potentially whether transferred knowledge of multiple languages can boost the performance of a morphology model even more.